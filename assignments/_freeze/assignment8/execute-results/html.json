{
  "hash": "410c98a4df75c8f0b4951800083f91a9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 8\"\nsubtitle: \"LOO-CV model comparison\"\nauthor: \"Aki Vehtari et al.\"\nformat:\n  html:\n    toc: true\n    code-tools: true\n    code-line-numbers: true\n    number-sections: true\n    mainfont: Georgia, serif\n    page-layout: article\neditor: source\nfilters:\n  - includes/assignments.lua \n  - includes/include-code-files.lua \n---\n\n\n\n\n# General information\n\n**The maximum amount of points from this assignment is 6.**\n\nWe have prepared a **quarto template specific to this assignment ([html](template8.html), [qmd](https://avehtari.github.io/BDA_course_Aalto/assignments/template8.qmd), [pdf](template8.pdf))** to help you get started. \n\n\n:::{.aalto}\nWe recommend you use [jupyter.cs.aalto.fi](https://jupyter.cs.aalto.fi) or the [docker container](docker.html).\n:::\n\n\n:::{.hint}\n**Reading instructions**:\n\n- [**The reading instructions for BDA3 Chapter 6**](../BDA3_notes.html#ch6) (posterior predictive checking).\n- [**The reading instructions for BDA3 Chapter 7**](../BDA3_notes.html#ch7) (predictive performance).\n- The [‘loo‘ package vignette on the basics of LOO](https://mc-stan.org/loo/articles/loo2-with-rstan.html)\nshows an example of how to modify Stan code and use the package with Stan models.\n- Also read\nabout PSIS-LOO in the [PSIS-LOO paper](https://link.springer.com/article/10.1007/s11222-016-9696-4).\n- [CV-FAQ](https://avehtari.github.io/modelselection/CV-FAQ.html) includes a lot of informative answers to frequent questions and misconceptions.\n\n\n**Grading instructions:** \n\nThe grading will be done in peergrade. \nAll grading questions and evaluations for this assignment are contained within this document\nin the collapsible **Rubric** blocks.\n\n\n**Installing and using `CmdStanR`:** \n\nSee the [Stan\ndemos](https://avehtari.github.io/BDA_course_Aalto/demos.html) on how to\nuse Stan in R (or Python).\n[Aalto JupyterHub](https://Aalto JupyterHub) has working R and\nCmdStanR/RStan environment and is probably the easiest way to use Stan.\n* To use CmdStanR in [Aalto JupyterHub](https://Aalto JupyterHub):<br>\n  `library(cmdstanr)`<br>\n  `set_cmdstan_path('/coursedata/cmdstan')`\n\nThe Aalto Ubuntu desktops also have the necessary libraries installed.]{.aalto}\n\nTo install Stan on your laptop, run 'install.packages(\\\"cmdstanr\\\",\nrepos = c(\\\"https://mc-stan.org/r-packages/\\\", getOption(\\\"repos\\\")))'\nin R. If you encounter problems, see additional answers in\n[**FAQ**](https://avehtari.github.io/BDA_course_Aalto/FAQ.html). [If you\ndon't succeed in short amount of time, it is probably easier to use\n[Aalto JupyterHub](https://Aalto JupyterHub).]{.aalto}\n\n[If you use `Aalto JupyterHub`, all necessary packages have been\npre-installed.]{.aalto} In your laptop, install package `cmdstanr`. Installation\ninstructions on Linux, Mac and Windows can be found at\n<https://mc-stan.org/cmdstanr/>. Additional useful packages are `loo`,\n`bayesplot` and `posterior` (but you don't need these in this\nassignment). For Python users, `PyStan`, `CmdStanPy`, and `ArviZ`\npackages are useful.\n\nStan manual can be found at <https://mc-stan.org/users/documentation/>.\nFrom this website, you can also find a lot of other useful material\nabout Stan.\n\nIf you edit files ending `.stan` in RStudio, you can click \"Check\" in\nthe editor toolbar to make syntax check. This can significantly speed-up\nwriting a working Stan model.\n\n\n:::\n\n\n\n::: {.callout-important collapse=false}\n# Reporting accuracy\n\n**For posterior statistics of interest, only\nreport digits that are not completely random based on the Monte Carlo standard error (MCSE).**\n\n*Example:* If you estimate $E(\\mu) \\approx 1.234$ with MCSE($E(\\mu)$)\n= 0.01, then the true expectation is likely to be between $1.204$ and\n$1.264$, it makes sense to report $E(\\mu) \\approx 1.2$.\n\nSee Lecture video 4.1, [the chapter\nnotes](../BDA3_notes.html#ch10),\nand [a case\nstudy](https://avehtari.github.io/casestudies/Digits/digits.html) for\nmore information.\n:::\n\n\n::: {.callout-tip collapse=true}\n## Further information\n\n- The recommended tool in this course is R (with the IDE RStudio).\n- Instead of installing R and RStudio on you own computer, see [**how\n  to use R and RStudio remotely**](https://avehtari.github.io/BDA_course_Aalto/FAQ.html#How_to_use_R_and_RStudio_remotely).\n- If you want to install R and RStudio locally,\n  download [R and RStudio](https://posit.co/download/rstudio-desktop/).\n- There are tons of tutorials, videos and introductions to R and\n  RStudio online. You can find some initial hints from [**RStudio\n  Education pages**](https://education.rstudio.com/).\n- When working with R, we recommend writing the report using `quarto` and the provided template.\n  The template includes the formatting instructions and how to include code and figures.\n- Instead of `quarto`, you can use other software to make the PDF\n  report, but the the same instructions for formatting should be used.\n- Report all results in a single, **anonymous** \\*.pdf -file and\n  submit it in [**peergrade.io**](peergrade.io).\n- The course has its own R package `aaltobda` with data and\n  functionality to simplify coding. The package is pre-installed in JupyterHub.\n  To install the package on your own system, run\n  the following code (upgrade=\\\"never\\\" skips question about updating other packages):\n```{.r}\ninstall.packages(\"aaltobda\", repos = c(\"https://avehtari.r-universe.dev\", getOption(\"repos\")))\n```\n-   Many of the exercises can be checked automatically using the R\n    package `markmyassignment` (pre-installed in JupyterHub).\n    Information on how to install and use the\n    package can be found in [the `markmyassignment` documentation](https://cran.r-project.org/web/packages/markmyassignment/vignettes/markmyassignment.html).\n    There is no need to include `markmyassignment` results in the\n    report.\n-   Recommended additional self study exercises for each chapter in BDA3\n    are listed in the course web page. These will help to gain deeper understanding of the topic.\n-   Common questions and answers regarding installation and technical\n    problems can be found in [Frequently Asked Questions\n    (FAQ)](https://avehtari.github.io/BDA_course_Aalto/FAQ.html).\n-   Deadlines for all assignments can be found on the course web page\n    and in Peergrade. You can set email alerts for the deadlines in\n    Peergrade settings.\n-   You are allowed to discuss assignments with your friends, but it is\n    not allowed to copy solutions directly from other students or from\n    internet.\n-   You can copy, e.g., plotting code from the course demos,\n    but really try to solve the actual assignment problems with your own\n    code and explanations.\n-   Do not share your answers publicly.\n-   Do not copy answers from the internet or from previous years. We compare\n    the answers to the answers from previous years and to the answers\n    from other students this year.\n-   Use of AI is allowed on the course, but the most of the work needs to by the student, and you need to report\n    whether you used AI and in which way you used them (See [points 5 and 6 in Aalto guidelines for use of AI in teaching](https://www.aalto.fi/en/services/guidance-for-the-use-of-artificial-intelligence-in-teaching-and-learning-at-aalto-university)).\n-   All suspected plagiarism will be\n    reported and investigated. See more about the [**Aalto University\n    Code of Academic Integrity and Handling Violations\n    Thereof**](https://into.aalto.fi/display/ensaannot/Aalto+University+Code+of+Academic+Integrity+and+Handling+Violations+Thereof).\n-   Do not submit empty PDFs, almost empty PDFs, copy of the questions, nonsense generated by yourself or AI, as these are just\n    harming the other students as they can't do peergrading for the\n    empty or nonsense submissions. Violations of this rule will be\n    reported and investigated in the same way was plagiarism.\n-   If you have any suggestions or improvements to the course material,\n    please post in the course chat feedback channel, create an issue, or\n    submit a pull request to the public repository!\n\n\n:::\n\n\n\n::: {.rubric weight=7.5}\n\n* Can you open the PDF and it's not blank nor nonsense?\nIf the pdf is blank, nonsense, or something like only a copy of the questions, 1) report it as problematic in Peergrade-interface to get another report to review, and 2) send a message to TAs.\n* Is the report anonymous?\n\n:::\n\n\n\n\n\n\n# A hierarchical model for chicken weight time series\n\n## Exploratory data analysis\n\nIn the first part of this assignment, you will explore the dataset `ChickWeight`. In particular, you will see what information is recorded in the dataset, and how you can use visualisation to learn more about the dataset. More information can be found on the corresponding page of the [R documentation](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/ChickWeight).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(ChickWeight, 10)\n```\n:::\n\n\n\n\n:::{.subtask letter=a}\nCreate a histogram to explore the range of chicken weights. \nDescribe what you see in the plot. What is the qualitative range of the data?\n:::\n\n\n\n:::{.subrubric}\n* Does the plot look correct and is it readable?\n* Has it been stated that the data is **...**?\n:::\n\n:::{.subtask}\nPlot the weight of each chicken over time in a line plot. Add colours based on the diet.\nDescribe what you see in the plot. \n:::\n\n:::{.subrubric}\n* Does the plot look correct and is it readable?\n:::\n\n## Linear regression\n\nIn this section, you will build a model that predicts the weight of a\nchicken over time and depending on the diet. After sampling from the posteriors, you\nwill use posterior predictive checks to see how well the predictions\nmatch the observations. Then you will adjust the model by adding more\ncomplexity, and check again.\n\n::: {.subtask}\nUsing `brms`, implement a pooled linear regression with a normal\nmodel and `weight` as the predicted\nvariable using `Diet` and `Time` as predictors. Try to use weakly informative priors.\n:::\n::: {.hint}\nFor the prior on `Time`, consider how much the weight of a chicken (in grams) could possibly\nchange each day. For the priors on the effects of different diets,\nconsider how much average weight difference would be possible between\ndiets.\n\nNote that as `Diet` is a categorical variable, the priors need to be\nspecified for each category (apart from `Diet1` which is taken to be\nthe baseline).\n:::\n\n\n\n:::{.subrubric}\n* Is the brms-formula **...**?\n* Is the family **...**?\n* Are the prior standard deviations around **...**?\n:::\n\n\nNext, you can use the `bayesplot` package to check the posterior\npredictions in relation to the observed data using the [`pp_check` function](https://mc-stan.org/bayesplot/reference/pp_check.html). \nThe function plots the $y$ values, which are the observed data, \nand the $y_\\text{rep}$ values, which are replicated data sets from the\nposterior predictive distribution.\n\n:::{.subtask}\nPerform the posterior predictive check with the default arguments.\nWhat do you observe? Based on the plot, do the posterior predictions\nencapsulate the main features of the observed data? Point out any\nmajor differences between the predictions and the observed data.\nAnswer the following questions:\n\n* Are there qualitative differences between the observed data and the predicted data?\n* Do the observed data seem quantitatively similar?\n:::\n\n:::{.subrubric}\n* Does the plot look correct and is it readable?\n* Has it been recognized that the predicted data include **...**?\n* Has it been recognized that the observed and predicted data **...**?\n:::\n\nThe default density plot is not always informative, but `bayesplot`\nhas different settings that can be used to create plots more\nappropriate for specific data.\n\n:::{.subtask}\nCreate another plot with grouping to the PPC plot using the arguments\n`type = \"intervals_grouped\"` and `group = \"Diet\"`.\nWhat do you observe? Point out any major differences\nbetween the predictions and the observed data.\nBased on your visualisations, how could the model be improved?\n:::\n\n:::{.subrubric}\n* Does the plot look correct and is it readable?\n* Is there at least one reasonable way to improve the model, e.g. **...**?\n:::\n\n## Log-normal linear regression\n\nBased on the identified issues from the posterior predictive check,\nthe model can be improved. It is advisable to change only one or a few\nthings about a model at once. At this stage, focus on changing the\nobservation model family to better account for the observed data.\n\nOne option is to use the lognormal observation model, which only allows\npositive values. In `brms` you can change the observation model family\nto this by setting the argument `family = \"lognormal\"`.\nNote that when using the log-normal observation model, the regression\ncoefficients represent the change in the log weight of a chicken. The\npriors have been adjusted accordingly in the template.\n\n::: {.subtask}\nAdjust the model, sample from the posterior and create the same two posterior predictive\ncheck plots. Comment on your observations. Does the new model better\ncapture some aspects of the data?\n:::\n::: {.hint}\n:::\n\n:::{.subrubric}\n* Do the plots look correct and are they readable?\n* Has it been recognized that the fit to data is **...**?\n:::\n\n## Hierarchical log-normal linear regression\n\nThe model can further be improved by directly considering potential\ndifferences in growth rate for individual chicken. Some chickens may\ninnately grow faster than others, and this difference can be included\nby including both population and group level effects in to the model.\n\nTo include a group effect in `brms`, the code `+\n(predictor|group)` can be added to the model formula. In this case,\nthe predictor is `Time` and the group is `Chick`.\n\n::: {.subtask}\nCreate the same two plots as for the previous models. Comment on what\nyou see. Do the predictions seem to better capture the observed data?\nAre there remaining discrepancies between the predictions and observed\ndata that could be addressed?\n:::\n\n:::{.subrubric}\n* Do the plots look correct and are they readable?\n* Has it been recognized that the fit to data is **...**?\n:::\n\n:::{.subtask}\nHave you encountered any convergence issues in the above models? Report and comment.\n:::\n:::{.subrubric}\n* Has there been a potentially brief discussion of the standard convergence criteria (Rhat, ESS, divergent transitions) for all models?\n:::\n\n## Model comparison using the ELPD\n\nThere are many ways of comparing models[^footnote1]. Commonly, we evaluate point predictions, such as the mean of the predictive distribution[^footnote2], or accuracy of the whole posterior predictive. Whether we prioritise point or density predictive accuracy may serve different purposes and lead to different outcomes for model choice [^footnote3]. It is common, however, to report predictive accuracy via log-scores and point-predictive accuracy via root-mean-squared-error based on the empirical average of the predictive distribution. To cross-validate both metrics on left out observations without need to sample from each leave-one-out posterior, we use Pareto-smoothed importance sampling as discussed in the course materials (see Lecture 9). \n\nWe start comparing models based on the log-score. Use `loo::loo()` and `loo::loo_compare()` to quantify the differences in predictive performance.\n\n[^footnote1]: In principle, when comparing models based on accuracy in predictions or parameter estimation (if true parameter values are available to you, as e.g. in simulation studies), we want to use so called strictly proper scoring rules that will always indicate when a \"better\" model is better and the score reaches its uniquely defined best value at the \"true\" model, if it is also well defined. See [**Gneiting and Raftery, (2007)**](https://www.tandfonline.com/doi/abs/10.1198/016214506000001437) for and in depth treatment of this topic.\n\n[^footnote2]: NOT predictions based on the mean of the posterior parameters, but first generating the predictive distribution and then computing an average.\n\n[^footnote3]: For instance, a unimodal and bimodal predictive density may have the same expected value, but very different areas of high posterior density and therefore very different log-scores.\n\n:::{.subtask}\nAnswer the following questions using `loo`/`loo_compare`:\n\n* Which model has the best predictive performance? \n* Does the uncertainty influence the decision of which model is best?\n:::\n\n:::{.subrubric}\n* Do the results look correct and have they been presented in a readable way? They should be roughly **...**.\n* Has it been recognized that the best model is **...** and that the uncertainty is **...**?\n:::\n:::{.subtask}\nAssess whether the approximation to the LOO-CV distributions are reliable.\nConsult the $\\hat{k}$ statistic which informs on the reliability of PSIS computation in PSIS-LOO. \nPlot the $\\hat{k}$ values for each model against the data point ID and discuss.\nAre they as expected?\n:::\n:::{.hint}\nFor hierarchical models, it may be more important to think about how well the individual group is predicted and how many observations there are in a group compared to the number of parameters estimated. Also check out [CV-FAQ on high Pareto-$\\hat{k}$ values](https://avehtari.github.io/modelselection/CV-FAQ.html#17_What_to_do_if_I_have_high_Pareto_(hat{k})%E2%80%99s).\n:::\n\n:::{.subrubric}\n* Do the plots look correct and are they readable?\n* Has it been explained why the $\\hat{k}$ values are highest for the **...**? **...**.\n:::\n:::{.subtask}\nPerform a PPC for the hierarchical model for \n\n* a few of the chickens with the highest $\\hat{k}$ values and\n* a few of the chickens with the lowest $\\hat{k}$ values\n\nusing the code in the template. What do you observe?\n:::\n\n:::{.subrubric}\n* Does the plot look correct and is it readable?\n* Has it been recognized that the chickens with high $\\hat{k}$ values **...**?\n:::\n\n## Model comparison using the RMSE\n\n:::{.subtask}\nUse the function in the template to compare the RMSE and the LOO-RMSE for the three models.\nExplain the difference between the RMSE and the LOO-RMSE in 1--3 sentences. Is one generally lower than the other? Why?\n:::\n\n:::{.subrubric}\n* Do the results look correct and have they been presented in a readable way? They should be roughly: **...**.\n* Has it been recognized that the RMSE is **...** than the LOO-RMSE because **...**?\n:::\n\n\n# Overall quality of the report\n\n::: {.rubric weight=7.5}\n\n* Does the report include comment on whether AI was used, and if AI was used, explanation on how it was used?\n    - No\n    - Yes\n* Does the report follow the formatting instructions?\n    - Not at all\n    - Little\n    - Mostly\n    - Yes\n* In case the report doesn't fully follow the general and formatting instructions, specify the instructions that have not been followed. If applicable, specify the page of the report, where this difference is visible. This will help the other student to improve their reports so that they are easier to read and review.\nIf applicable, specify the page of the report, where this difference in formatting is visible.\n* Please also provide feedback on the presentation (e.g. text, layout, flow of the responses, figures, figure captions). Part of the course is practicing making data analysis reports. By providing feedback on the report presentation, other students can learn what they can improve or what they already did well. You should be able to provide constructive or positive feedback for all non-empty and non-nonsense reports. If you think the report is perfect, and you can’t come up with any suggestions how to improve, you can provide feedback on what you liked and why you think some part of the report is better than yours.\n\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}